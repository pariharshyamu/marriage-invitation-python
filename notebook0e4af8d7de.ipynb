{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.16","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":11105481,"sourceType":"datasetVersion","datasetId":5857}],"dockerImageVersionId":30920,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.applications import MobileNetV2\nfrom tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nimport os\n\n# ---- 1. TPU Configuration ----\ntry:\n    # Detect and initialize TPU\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU:', tpu.cluster_spec().as_dict()['worker'])\n    \n    # Connect to TPU cluster\n    tf.config.experimental_connect_to_cluster(tpu)\n    \n    # Initialize TPU system\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    \n    # Create distribution strategy for TPU\n    strategy = tf.distribute.TPUStrategy(tpu)\n    \n    print(\"TPU detected and configured successfully!\")\n    print(f\"Number of accelerators: {strategy.num_replicas_in_sync}\")\n    \n    # Set mixed precision policy for TPU\n    tf.keras.mixed_precision.set_global_policy('mixed_bfloat16')\n    print(\"Using mixed precision bfloat16 policy for TPU\")\n    \nexcept ValueError:\n    print(\"No TPU detected, falling back to GPU/CPU.\")\n    # Fallback to GPU configuration\n    physical_devices = tf.config.list_physical_devices('GPU')\n    if physical_devices:\n        tf.config.experimental.set_memory_growth(physical_devices[0], True)\n        tf.keras.mixed_precision.set_global_policy('mixed_float16')\n        strategy = tf.distribute.MirroredStrategy()\n        print(f\"Using GPU with {strategy.num_replicas_in_sync} device(s)\")\n    else:\n        strategy = tf.distribute.get_strategy()\n        print(\"Using CPU\")\n\n# ---- 2. Kaggle Dataset & Model Paths ----\n# Kaggle input directory typically contains the dataset\nDATASET_PATH = \"../input/fruits\"  # Adjust if needed\nMODEL_PATH = \"./mobilenet_fruits360_optimized.h5\"\nCHECKPOINT_PATH = \"./checkpoints/model_checkpoint.h5\"\n\n# Create checkpoint directory if it doesn't exist\nos.makedirs(os.path.dirname(CHECKPOINT_PATH), exist_ok=True)\n\n# ---- 3. Check if Dataset exists ----\nif not os.path.exists(DATASET_PATH):\n    print(f\"Dataset not found at {DATASET_PATH}\")\n    print(\"Please make sure to add the 'fruits-360-dataset' to your Kaggle notebook.\")\n    # Check common alternate locations in Kaggle\n    alt_paths = [\n        \"../input/fruits-360\",\n        \"../input/fruit-images-for-object-detection\",\n        \"../input/fruits-360_dataset\"\n    ]\n    for path in alt_paths:\n        if os.path.exists(path):\n            print(f\"Found dataset at alternate location: {path}\")\n            DATASET_PATH = path\n            break\n\n# ---- 4. Optimized Data Loading & Augmentation ----\n# Increase batch size for TPU performance\nBATCH_SIZE = 512  # TPUs perform better with larger batch sizes\nIMG_SIZE = 96  \n\n# Validation split reduced to 0.1\ntrain_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n    rescale=1.0/255,\n    rotation_range=20,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    horizontal_flip=True,\n    zoom_range=0.2,\n    validation_split=0.1,\n    preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input\n)\n\n# Use TF data API for more efficient data loading\ndef create_dataset(generator, subset):\n    dataset = tf.data.Dataset.from_generator(\n        lambda: generator.flow_from_directory(\n            DATASET_PATH,\n            target_size=(IMG_SIZE, IMG_SIZE),\n            batch_size=BATCH_SIZE,\n            class_mode=\"categorical\",\n            subset=subset,\n            shuffle=True if subset == \"training\" else False\n        ),\n        output_signature=(\n            tf.TensorSpec(shape=(None, IMG_SIZE, IMG_SIZE, 3), dtype=tf.float32),\n            tf.TensorSpec(shape=(None, None), dtype=tf.float32)\n        )\n    )\n    \n    # TPU optimization: cache, optimize for TPU processing, prefetch\n    dataset = dataset.cache()\n    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n    \n    return dataset\n\n# Try to find the correct training data directory\n# Kaggle's Fruits-360 dataset might have different directory structures\npossible_data_dirs = [\n    DATASET_PATH,\n    os.path.join(DATASET_PATH, \"fruits-360\"),\n    os.path.join(DATASET_PATH, \"fruits-360_dataset\", \"fruits-360\"),\n    os.path.join(DATASET_PATH, \"Training\")\n]\n\ndata_dir_found = False\nfor dir_path in possible_data_dirs:\n    if os.path.exists(dir_path):\n        # Check if this directory contains subdirectories (classes)\n        subdirs = [f for f in os.listdir(dir_path) if os.path.isdir(os.path.join(dir_path, f))]\n        if subdirs:\n            print(f\"Found valid dataset directory: {dir_path}\")\n            DATASET_PATH = dir_path\n            data_dir_found = True\n            break\n\nif not data_dir_found:\n    print(\"Warning: Could not automatically find the correct dataset directory structure.\")\n    print(\"Please verify the dataset path and structure manually.\")\n\n# Now create datasets\ntry:\n    train_ds = create_dataset(train_datagen, \"training\")\n    val_ds = create_dataset(train_datagen, \"validation\")\n\n    # Get number of classes\n    temp_generator = train_datagen.flow_from_directory(\n        DATASET_PATH, target_size=(IMG_SIZE, IMG_SIZE), batch_size=1, class_mode=\"categorical\", subset=\"training\"\n    )\n    num_classes = len(temp_generator.class_indices)\n    steps_per_epoch = temp_generator.samples // BATCH_SIZE\n    \n    # Get validation samples count\n    val_generator = train_datagen.flow_from_directory(\n        DATASET_PATH, target_size=(IMG_SIZE, IMG_SIZE), batch_size=1, class_mode=\"categorical\", subset=\"validation\"\n    )\n    validation_steps = val_generator.samples // BATCH_SIZE\n    validation_steps = max(1, validation_steps)  # Ensure at least 1 step\n\n    # Print dataset statistics\n    print(f\"Number of classes: {num_classes}\")\n    print(f\"Training samples: {temp_generator.samples}\")\n    print(f\"Validation samples: {val_generator.samples}\")\n    print(f\"Steps per epoch: {steps_per_epoch}\")\n    print(f\"Validation steps: {validation_steps}\")\n    \nexcept Exception as e:\n    print(f\"Error setting up dataset: {e}\")\n    print(\"\\nTrying alternate dataset structure (Training/Test directories)...\")\n    \n    # Try alternate directory structure common in Kaggle datasets\n    TRAIN_DIR = os.path.join(DATASET_PATH, \"Training\")\n    TEST_DIR = os.path.join(DATASET_PATH, \"Test\")\n    \n    if os.path.exists(TRAIN_DIR) and os.path.exists(TEST_DIR):\n        # For the alternative structure, we'll manually split the training data 90/10\n        from sklearn.model_selection import train_test_split\n        import numpy as np\n        \n        # Set up with manual validation split\n        train_datagen_alt = tf.keras.preprocessing.image.ImageDataGenerator(\n            rescale=1.0/255,\n            rotation_range=20,\n            width_shift_range=0.2,\n            height_shift_range=0.2,\n            horizontal_flip=True,\n            zoom_range=0.2,\n            preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input\n        )\n        \n        # Separate validation generator without augmentation\n        val_datagen_alt = tf.keras.preprocessing.image.ImageDataGenerator(\n            rescale=1.0/255,\n            preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input\n        )\n        \n        # Get all image files and classes\n        import glob\n        \n        all_images = []\n        all_labels = []\n        class_dirs = [d for d in os.listdir(TRAIN_DIR) if os.path.isdir(os.path.join(TRAIN_DIR, d))]\n        class_to_idx = {cls_name: i for i, cls_name in enumerate(class_dirs)}\n        \n        for cls_name in class_dirs:\n            cls_path = os.path.join(TRAIN_DIR, cls_name)\n            for img_path in glob.glob(os.path.join(cls_path, \"*.jpg\")):\n                all_images.append(img_path)\n                all_labels.append(class_to_idx[cls_name])\n        \n        # Split with 90% training, 10% validation\n        train_imgs, val_imgs, train_labels, val_labels = train_test_split(\n            all_images, all_labels, test_size=0.1, stratify=all_labels, random_state=42\n        )\n        \n        print(f\"Manual split created: {len(train_imgs)} training images, {len(val_imgs)} validation images\")\n        \n        # Create custom generators from file lists - optimized for TPU\n        from tensorflow.keras.utils import to_categorical\n        \n        def create_tpu_dataset_from_files(image_paths, labels, num_classes, is_training=True):\n            def _parse_function(filename, label):\n                image_string = tf.io.read_file(filename)\n                image = tf.image.decode_jpeg(image_string, channels=3)\n                image = tf.image.resize(image, [IMG_SIZE, IMG_SIZE])\n                image = tf.keras.applications.mobilenet_v2.preprocess_input(image)\n                \n                # Apply random augmentations if training\n                if is_training:\n                    image = tf.image.random_flip_left_right(image)\n                    image = tf.image.random_brightness(image, 0.2)\n                    image = tf.image.random_contrast(image, 0.8, 1.2)\n                \n                label = tf.one_hot(label, depth=num_classes)\n                return image, label\n            \n            # Create a dataset from the file paths and labels\n            dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n            \n            # Shuffle if training\n            if is_training:\n                dataset = dataset.shuffle(buffer_size=len(image_paths))\n            \n            # Parse files and augment\n            dataset = dataset.map(_parse_function, num_parallel_calls=tf.data.AUTOTUNE)\n            \n            # Batch and prefetch\n            dataset = dataset.batch(BATCH_SIZE)\n            dataset = dataset.cache()\n            dataset = dataset.prefetch(tf.data.AUTOTUNE)\n            \n            return dataset\n            \n        num_classes = len(class_dirs)\n        \n        # Create TPU-optimized datasets\n        train_ds = create_tpu_dataset_from_files(train_imgs, train_labels, num_classes, is_training=True)\n        val_ds = create_tpu_dataset_from_files(val_imgs, val_labels, num_classes, is_training=False)\n        \n        steps_per_epoch = (len(train_imgs) + BATCH_SIZE - 1) // BATCH_SIZE  # Ceiling division\n        validation_steps = (len(val_imgs) + BATCH_SIZE - 1) // BATCH_SIZE\n        \n        print(f\"Alternative dataset structure processed for TPU:\")\n        print(f\"Number of classes: {num_classes}\")\n        print(f\"Training samples: {len(train_imgs)}\")\n        print(f\"Validation samples: {len(val_imgs)}\")\n        print(f\"Steps per epoch: {steps_per_epoch}\")\n        print(f\"Validation steps: {validation_steps}\")\n    else:\n        raise Exception(\"Could not find valid dataset structure. Please check your dataset path.\")\n\n# ---- 5. Define Model Creation Function within Strategy Scope ----\n# TPU Strategy Scope for model creation\ndef create_model():\n    # Use smaller input size and alpha parameter for faster inference\n    base_model = MobileNetV2(\n        weights=\"imagenet\", \n        include_top=False, \n        input_shape=(IMG_SIZE, IMG_SIZE, 3),\n        alpha=0.75  # Smaller network (75% of filters)\n    )\n\n    # Freeze base model for initial training\n    base_model.trainable = False\n\n    # Efficient Model Head\n    x = base_model.output\n    x = GlobalAveragePooling2D(name=\"gap\")(x)\n    x = Dense(128, activation=\"relu\", name=\"dense_1\")(x)\n    x = Dropout(0.4, name=\"dropout_1\")(x)\n    # Force float32 output for TPU compatibility\n    output_layer = Dense(num_classes, activation=\"softmax\", dtype='float32', name=\"output\")(x)\n\n    model = Model(inputs=base_model.input, outputs=output_layer)\n    \n    # Learning rate schedule for better convergence\n    initial_learning_rate = 0.001\n    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n        initial_learning_rate,\n        decay_steps=steps_per_epoch*2,\n        decay_rate=0.9,\n        staircase=True\n    )\n\n    # TPU-optimized compilation\n    model.compile(\n        optimizer=keras.optimizers.Adam(learning_rate=lr_schedule),\n        loss=\"categorical_crossentropy\",\n        metrics=[\"accuracy\", tf.keras.metrics.TopKCategoricalAccuracy(k=3, name=\"top3_acc\")]\n    )\n    \n    return model, base_model\n\n# Create model inside TPU strategy scope\nwith strategy.scope():\n    model, base_model = create_model()\n\n# Summary of model architecture\nprint(\"Model Architecture Summary:\")\nmodel.summary()\n\n# ---- 8. Callbacks for Better Training ----\n# Ensure TPU compatibility for callbacks\ncallbacks = [\n    # Reduced patience for early stopping\n    EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True),\n    \n    # Save model checkpoints\n    ModelCheckpoint(\n        filepath=CHECKPOINT_PATH,\n        save_best_only=True,\n        monitor='val_accuracy',\n        mode='max'\n    ),\n    \n    # Reduced patience for learning rate reduction\n    ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=1, min_lr=1e-6),\n    \n    # TensorBoard logging\n    tf.keras.callbacks.TensorBoard(log_dir=\"./logs\", histogram_freq=1)\n]\n\n# ---- 9. Initial Training Phase ----\nprint(\"\\nStarting initial training phase on TPU...\")\nhistory = model.fit(\n    train_ds,\n    epochs=5,\n    validation_data=val_ds,\n    callbacks=callbacks,\n    steps_per_epoch=steps_per_epoch,\n    validation_steps=validation_steps\n)\n\n# ---- 10. Selective Fine-Tuning ----\nprint(\"\\nStarting fine-tuning phase...\")\n# Need to update the model inside TPU strategy scope\nwith strategy.scope():\n    # Unfreeze the last block of the MobileNetV2 model\n    for layer in base_model.layers[-12:]:\n        layer.trainable = True\n\n    # Count trainable parameters\n    trainable_count = sum(tf.keras.backend.count_params(w) for w in model.trainable_weights)\n    non_trainable_count = sum(tf.keras.backend.count_params(w) for w in model.non_trainable_weights)\n    print(f\"Trainable parameters: {trainable_count:,}\")\n    print(f\"Non-trainable parameters: {non_trainable_count:,}\")\n\n    # Use a much smaller learning rate for fine-tuning\n    model.compile(\n        optimizer=keras.optimizers.Adam(learning_rate=1e-5),\n        loss=\"categorical_crossentropy\",\n        metrics=[\"accuracy\", tf.keras.metrics.TopKCategoricalAccuracy(k=3, name=\"top3_acc\")]\n    )\n\n# Fine-tune with early stopping\nhistory_finetune = model.fit(\n    train_ds,\n    epochs=5,\n    validation_data=val_ds,\n    callbacks=callbacks,\n    steps_per_epoch=steps_per_epoch,\n    validation_steps=validation_steps\n)\n\n# ---- 11. Evaluation ----\nprint(\"\\nEvaluating model on validation set...\")\nevaluation = model.evaluate(val_ds, steps=validation_steps)\nprint(f\"Final validation loss: {evaluation[0]:.4f}\")\nprint(f\"Final validation accuracy: {evaluation[1]:.4f}\")\nprint(f\"Final validation top-3 accuracy: {evaluation[2]:.4f}\")\n\n# ---- 12. Save Models ----\n# Save the Keras model to Kaggle's output directory\nmodel.save(MODEL_PATH)\nprint(f\"Saved Keras model to {MODEL_PATH}\")\n\n# Convert to TensorFlow Lite for deployment\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\ntflite_model = converter.convert()\n\n# Save the TF Lite model\ntflite_path = os.path.join(os.path.dirname(MODEL_PATH), 'model.tflite')\nwith open(tflite_path, 'wb') as f:\n    f.write(tflite_model)\nprint(f\"Saved TFLite model to {tflite_path}\")\n\n# ---- 13. Output class indices for later use ----\n# Save the class indices for inference\nimport json\n\n# Handle different dataset structures\nif 'temp_generator' in locals():\n    class_indices = temp_generator.class_indices\nelif 'class_to_idx' in locals():\n    class_indices = {cls: idx for cls, idx in class_to_idx.items()}\nelse:\n    class_indices = {}\n\nwith open('class_indices.json', 'w') as f:\n    json.dump(class_indices, f)\nprint(\"Saved class indices to class_indices.json\")\n\n# ---- 14. Sample prediction code ----\nprint(\"\\nSample code for making predictions:\")\nprint(\"\"\"\n# Code to load and use the model for prediction\nimport tensorflow as tf\nimport numpy as np\nfrom tensorflow.keras.preprocessing import image\nimport json\n\n# Load the model\nmodel = tf.keras.models.load_model('mobilenet_fruits360_optimized.h5')\n\n# Load class indices\nwith open('class_indices.json', 'r') as f:\n    class_indices = json.load(f)\n    \n# Invert the dictionary to map indices to class names\nidx_to_class = {v: k for k, v in class_indices.items()}\n\n# Function to preprocess and predict\ndef predict_fruit(img_path):\n    img = image.load_img(img_path, target_size=(96, 96))\n    img_array = image.img_to_array(img)\n    img_array = np.expand_dims(img_array, axis=0)\n    img_array = tf.keras.applications.mobilenet_v2.preprocess_input(img_array)\n    \n    predictions = model.predict(img_array)\n    predicted_class_idx = np.argmax(predictions[0])\n    confidence = predictions[0][predicted_class_idx] * 100\n    \n    return idx_to_class[predicted_class_idx], confidence\n\n# Example usage\n# fruit_name, confidence = predict_fruit('path/to/your/fruit/image.jpg')\n# print(f'Predicted fruit: {fruit_name} with {confidence:.2f}% confidence')\n\"\"\")\n\nprint(\"\\nTraining and optimization with TPU complete!\")\n\n# ---- 15. Create a simple visualization of training history ----\ntry:\n    import matplotlib.pyplot as plt\n    \n    # Plot training & validation accuracy\n    plt.figure(figsize=(12, 4))\n    \n    plt.subplot(1, 2, 1)\n    plt.plot(history.history['accuracy'])\n    plt.plot(history.history['val_accuracy'])\n    plt.title('Model Accuracy (Initial Training)')\n    plt.ylabel('Accuracy')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Validation'], loc='upper left')\n    \n    plt.subplot(1, 2, 2)\n    plt.plot(history_finetune.history['accuracy'])\n    plt.plot(history_finetune.history['val_accuracy'])\n    plt.title('Model Accuracy (Fine-tuning)')\n    plt.ylabel('Accuracy')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Validation'], loc='upper left')\n    \n    plt.tight_layout()\n    plt.savefig('training_history.png')\n    print(\"Saved training history visualization to 'training_history.png'\")\nexcept Exception as e:\n    print(f\"Could not create visualization: {e}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-20T18:47:37.160886Z","iopub.execute_input":"2025-03-20T18:47:37.161243Z","iopub.status.idle":"2025-03-20T18:47:37.398971Z","shell.execute_reply.started":"2025-03-20T18:47:37.161214Z","shell.execute_reply":"2025-03-20T18:47:37.397886Z"}},"outputs":[{"name":"stdout","text":"No TPU detected, falling back to GPU/CPU.\nUsing CPU\nDataset not found at ../input/fruits-360-dataset\nPlease make sure to add the 'fruits-360-dataset' to your Kaggle notebook.\nWarning: Could not automatically find the correct dataset directory structure.\nPlease verify the dataset path and structure manually.\nError setting up dataset: [Errno 2] No such file or directory: '../input/fruits-360-dataset'\n\nTrying alternate dataset structure (Training/Test directories)...\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[4], line 139\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;66;03m# Get number of classes\u001b[39;00m\n\u001b[0;32m--> 139\u001b[0m temp_generator \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_datagen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflow_from_directory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mDATASET_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mIMG_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIMG_SIZE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcategorical\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtraining\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m num_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(temp_generator\u001b[38;5;241m.\u001b[39mclass_indices)\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/keras/src/legacy/preprocessing/image.py:1138\u001b[0m, in \u001b[0;36mImageDataGenerator.flow_from_directory\u001b[0;34m(self, directory, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio)\u001b[0m\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mflow_from_directory\u001b[39m(\n\u001b[1;32m   1121\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1122\u001b[0m     directory,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1136\u001b[0m     keep_aspect_ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1137\u001b[0m ):\n\u001b[0;32m-> 1138\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDirectoryIterator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1139\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1140\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1141\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1142\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolor_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolor_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1143\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_aspect_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_aspect_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1144\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclasses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclasses\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1145\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1146\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1150\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_to_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_to_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1151\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1152\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1153\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_links\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_links\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1154\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1155\u001b[0m \u001b[43m        \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1157\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/keras/src/legacy/preprocessing/image.py:453\u001b[0m, in \u001b[0;36mDirectoryIterator.__init__\u001b[0;34m(self, directory, image_data_generator, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio, dtype)\u001b[0m\n\u001b[1;32m    452\u001b[0m classes \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 453\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m subdir \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m):\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(directory, subdir)):\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../input/fruits-360-dataset'","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[4], line 262\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation steps: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalidation_steps\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 262\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not find valid dataset structure. Please check your dataset path.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# ---- 5. Define Model Creation Function within Strategy Scope ----\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# TPU Strategy Scope for model creation\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate_model\u001b[39m():\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;66;03m# Use smaller input size and alpha parameter for faster inference\u001b[39;00m\n","\u001b[0;31mException\u001b[0m: Could not find valid dataset structure. Please check your dataset path."],"ename":"Exception","evalue":"Could not find valid dataset structure. Please check your dataset path.","output_type":"error"}],"execution_count":4}]}